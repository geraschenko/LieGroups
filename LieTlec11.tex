 \stepcounter{lecture}
 \setcounter{lecture}{11}
 \sektion{Lecture 11 - Engel's Theorem and Lie's Theorem} \index{Serganova, Vera|(}

 In the next ten lectures, we will cover
 \begin{enumerate}
 \item Classification of semisimple Lie algebras. This will include root systems and
       Dynkin diagrams.
%    When you study groups, there are solvable and nilpotent groups ... there is a
%    similar classification for Lie algebras.

 \item Representation theory of semisimple Lie algebras and the Weyl character formula.

 \item Compact connected Lie Groups.
 \end{enumerate}
 A reference for this material is Fulton and Harris \cite{FulHar}.

 The first part is purely algebraic: we will study Lie algebras. $\g$ will be a Lie
 algebra, usually finite dimensional, over a field $k$ (usually of characteristic 0).

 Any Lie algebra $\g$ contains the ideal $\D(\g) = [\g,\g]$, the vector subspace
 generated by elements of the form $[X,Y]$ for $X,Y\in \g$.
 \begin{exercise}
   Show that $\D\g$ is an ideal in $\g$.
   \begin{solution}
     $[\g,\D\g]\subseteq [\g,\g] = \D\g$, so $\D\g$ is an ideal.
   \end{solution}
 \end{exercise}
 \begin{exercise} \label{lec11hardEx}
   Let $G$ be a simply connected Lie group with Lie algebra $\g$. Then $[G,G]$ is the
   subgroup of $G$ generated by elements of the form $ghg^{-1}h^{-1}$ for $g,h\in G$.
   Show that $[G,G]$ is a connected closed normal Lie subgroup of $G$, with Lie
   algebra $\D\g$.
  \begin{solution}
   $[G,G]$ is normal because $r[g,h]r^{-1}=[rgr^{-1},rhr^{-1}]$. To see
   that $[G,G]$ is connected, let $\gamma_{gh}:[0,1]\to G$ be a path from $g$ to $h$.
   Then $t\mapsto g\gamma(t)g^{-1}\gamma(t)^{-1}$ is a path in $[G,G]$ from
   the identity to $[g,h]$. Since all the generators of $[G,G]$ are connected
   to $e\in G$ by paths, all of $[G,G]$ is connected to $e$.

   Now we show that the Lie algebra of $[G,G]$ is $\D\g$. Consider the Lie algebra
   homomorphism $\pi:\g\to \g/\D\g$. Since $G$ is simply connected, Theorem
   \ref{lec04T:4} says there is a Lie group homomorphism $p:G\to H$ lifting $\pi$.
   \[\xymatrix{
    \D\g \ar[r] &\g \ar[r]^\pi \ar[d]_\exp & \g/\D\g\ar[d]^\exp\\
    [G,G]\ar[r] & G\ar@{.>}[r]^p & **[r] H\cong \RR^n
   }\]
   where $H$ is the simply connected Lie group with Lie algebra $\g/\D\g$. Note that
   the Lie algebra of the kernel of $p$ must be contained in $\ker \pi = \D\g$. Also,
   $\g/\D\g$ is abelian, so $H$ is abelian, so $[G,G]$ is in the kernel of $p$. This
   shows that $\lie([G,G])\subseteq \D\g$.

   To see that $\D\g\subseteq \lie([G,G])$, assume that $\g\subseteq \gl(V)$. Then for
   $X,Y\in \g$ consider the path $\gamma(t) =
   \exp(X\sqrt{t})\exp(Y\sqrt{t})\exp(-X\sqrt{t})\exp(-Y\sqrt{t})$ in $[G,G]$:
   \begin{align*}
     \gamma(t) &= \left(1+X\sqrt{t}+\frac{1}{2}X^2t+\cdots\right)
                 \left(1+Y\sqrt{t}+\frac{1}{2}Y^2t+\cdots\right) \times \\
               & \hspace{5em}
                 \left(1-X\sqrt{t}+\frac{1}{2}X^2t+\cdots\right)
                 \left(1-Y\sqrt{t}+\frac{1}{2}Y^2t+\cdots\right) \\
               &= 1+\sqrt{t}(X+Y-X-Y) + \\
               &\hspace{2.5em} t(XY-X^2-XY-YX -Y^2 +X^2+Y^2) +\cdots\\
               &=1+ t[X,Y] + O(t^{3/2})
   \end{align*}
   So $\gamma'(0)=[X,Y]$. This shows that $[G,G]$ is a connected component of the
   kernel of $p$.

   Since $[G,G]$ is a connected component of $p^{-1}(0)$, it is closed in $G$.
  \end{solution}
 \end{exercise}

   \begin{warning} Exercise \ref{lec11hardEx} is a tricky problem. Here are some
   potential pitfalls:
     \begin{enumerate}
       \item For $G$ connected, we do not necessarily know that the exponential map is
       surjective, because $G$ may not be complete. For example, $\exp: \sl_2(\CC)\to
       SL_2(\CC)$ is not surjective.\footnote{Assume $\matrix {-1}10{-1}$ is in the
       image, then its pre-image must have eigenvalues $(2n+1)i\pi$ and $-(2n+1)i\pi$
       for some integer $n$. So the pre-image has distinct eigenvalues, so it is
       diagonalizable. But that implies that $\matrix {-1}10{-1}$ is diagonalizable,
       contradiction.}

       \item If $H\subseteq G$ is a subgroup with Lie algebra $\h$, then $\h\subseteq
       \g$ closed is not enough to know that $H$ is closed in $G$. For example, take
       $G$ to be a torus, and $H$ to be a line with irrational slope.

       \item The statement is false if we relax the condition that $G$ is simply
       connected. Let
       \begin{align*}
          H &:= \left\{ \mbox{\scriptsize $\mat{1 & x & y\\0 & 1 & z\\0 & 0 &1}$}\right\}\times S^1\\
          K &:= \left\{ \Biggl(
            \mbox{\scriptsize $\mat{
              1 & 0 & n\\
              0 & 1 & 0\\
              0 & 0 & 1}$},c^n\Biggr)
          \biggm|n\in \ZZ \right\}
          \subseteq H
       \end{align*}
       where $c$ is an element of $S^1$ of infinite order. Then $K$ is normal in $H$
       and $G=H/K$ is a counterexample.
     \end{enumerate}
   \end{warning}

% If you believe these exercises, then the next definitions should make sense.

 \begin{definition}
   Define $\D^0\g=\g$, and $\D^n\g = [\D^{n-1}\g,\D^{n-1}\g]$. This is called the
   \emph{derived series}\index{derived series} of $\g$. We say $\g$ is
   \emph{solvable}\index{solvable} if $\D^n\g=0$ for some $n$ sufficiently large.
 \end{definition}
 \begin{definition}
   We can also define $\D_0\g=\g$, and $\D_n\g = [\g,\D_{n-1}\g]$. This is called the
   \emph{lower central series}\index{lower central series} of $\g$. We say that $\g$
   is \emph{nilpotent}\index{nilpotent} if $\D_n\g=0$ for some $n$ sufficiently large.
 \end{definition}

 Note that $\D_1\g=\D^1\g$ by $\D\g$. Solvable and nilpotent Lie algebras are
 hard to classify. Instead, we will do the classification of semisimple Lie algebras
 (see Definition \ref{lec11def:semisimple}).

 The following example is in some sense universal (see corollaries \ref{lec11Engelcor}
 and \ref{lec11Liecor}):
 \begin{example}
   Let $\gl(n)$\index{gl(n)@$\gl(n)$} be the Lie algebra of all $n\times n$ matrices,
   and let $\b$\index{b@$\b$}\index{upper triangular|see{$\b$}} be the subalgebra of
   upper triangular matrices. I claim that $\b$ is solvable. To see this, note that
   $\D\b$ is the algebra of \emph{strictly} upper triangular matrices, and in general,
   $\D^k\b$ has zeros on the main diagonal and the $2^{k-2}$ diagonals above the main
   diagonal (for $k\ge 2$). Let $\mathfrak{n}=\D\b$. You can check that $\mathfrak{n}$
   is in fact nilpotent.
 \end{example}

\noindent Useful facts about solvable/nilpotent Lie algebras:
 \index{useful facts about solvable and nilpotent Lie algebras|(idxbf}
 \begin{enumerate}
 \item \label{lec11Fexactsolv} If you have an exact sequence of Lie algebras
 \[
    0\to \a\to \g\to \g/\a \to 0
 \]
 then $\g$ is solvable if and only if $\a$ and $\g/\a$ are solvable.

 \item \label{lec11Fexactnil} If you have an exact sequence of Lie algebras
 \[
    0\to \a\to \g\to \g/\a \to 0
 \]
 then if $\g$ is nilpotent, so are $\a$ and $\g/\a$.

 \begin{warning}
   The converse is not true. Diagonal matrices $\mathfrak{d}$\index{d@$\mathfrak{d}$} is
   nilpotent, and we have
 \[
    0\to \mathfrak{n}\to \b\to \mathfrak{d}\to 0.
 \]
 Note that $\b$ is not nilpotent, because $\D\b=\D_n\b=\matrix{0}{\ast}{0}{0}$.
 \end{warning}

 \item \label{lec11Fsumsolv} If $\a,\b\subset \g$ are solvable ideals, then the sum
 $\a+\b$ is solvable. To see this, note that we have
 \[
    0\to \a\to \a+\b \to \underbrace{(\a+\b)/\a}_{\simeq \b/(\a\cap\b)} \to 0
 \]
 $\a$ is solvable by assumption, and $\b/(\a\cap\b)$ is a quotient of a solvable
 algebra, so it is solvable by (1). Applying (1) again, $\a+\b$ is solvable.

% \item[(1)] A subalgebra of a solvable (nilpotent) algebra is again solvable
% (nilpotent).
% \item[(2)] A quotient of a solvable (nilpotent) algebra is again solvable
% (nilpotent).
% \item[(3)] If you have a short exact sequence
% \[
%    0\to \a\to \g \to \mathfrak{f}\to 0
% \]
% with $\a$ and $\mathfrak{f}$ solvable, then $\g$ is also solvable. This doesn't work
% for nilpotent. Diagonal matrices is nilpotent, and we have
% \[
%    0\to \mathfrak{n}\to \b\to \mathfrak{d}\to 0
% \]
% To see that $\b$ is not nilpotent, notice that $\D\b=\matrix{0}{\ast}{0}{0}$, but
% then $\D_2\b = \matrix{0}{\ast}{0}{0}$.

 \item \label{lec11Fscalers} If $k\subseteq F$ is a field extension, with a Lie algebra
 $\g$ over $k$, we can make a Lie algebra $\g\otimes_k F$ over $F$. Note that
 $\g\otimes_k F$ is solvable (nilpotent) if and only if $\g$ is.
 \end{enumerate}
 \index{useful facts about solvable and nilpotent Lie algebras|)\idxbf}


 We will now prove Engel's theorem and Lie's theorem.

% They are as ubiquitous as Schur's lemma.

 For any Lie algebra $\g$, we have  the adjoint representation: $X\mapsto ad_X\in
 \gl(\g)$ given by $ad_X(Y)=[X,Y]$. If $\g$ is nilpotent, then $ad_X$ is a nilpotent
 operator for any $X\in \g$. The converse is also true as we will see shortly (Cor.\
 \ref{lec11Engelcor2}).

 \begin{theorem}[Engel's Theorem] \label{lec11Engel} \index{Engel's Theorem|idxbf}
   Let $\g\subseteq \gl(V)$, and assume that $X$ is nilpotent for any $X\in \g$. Then
   there is a vector $v\in V$ such that $\g \cdot v =0$.
 \end{theorem}
 Note that the theorem holds for any representation $\rho$ of $\g$ in which every
 element acts nilpotently; just replace $\g$ in the statement of the theorem by
 $\rho(\g)$.
 \begin{corollary}\label{lec11Engelcor}
   If $V$ is a representation of $\g$ in which every element acts nilpotently, then
   one can find $\{0\}=V_0\subsetneq V_1\subsetneq \cdots \subsetneq V_n=V$ a complete
   flag such that $\g(V_i)\subseteq V_{i-1}$. That is, there is a basis in which all
   of the elements of $\g$ are strictly upper triangular.
 \end{corollary}
 \begin{warning}
 Note that the theorem isn't true if you say ``suppose $\g$ is nilpotent'' instead of
 the right thing. For example, the set of diagonal matrices $\mathfrak{d}\subset
 \gl(V)$ is nilpotent.
 \end{warning}
 \begin{proof}
  Let's prove the theorem by induction on $\dim \g$.We first show that $\g$ has an
  ideal $\a$ of codimension 1. To see this, take a maximal proper subalgebra
  $\a\subset \g$. Look at the representation of $\a$ on the quotient space $\g/\a$.
  This representation, $\a\to \gl(\g/\a)$, satisfies the condition of the
  theorem,\footnote{For any $X\in \a$, since $X$ is nilpotent, $ad_X$ is also
  nilpotent.} so by induction, there is some $X\in \g$ such that $ad_\a(X)=0$ modulo
  $\a$. So $[\a,X]\subseteq \a$, so $\mathfrak{b} = kX\oplus \a$ is a new subalgebra of $\g$
  which is larger, so it must be all of $\g$. Thus, $\a$ must have had codimension 1.
  Therefore, $\a\subseteq \g$ is actually an ideal (because $[X,\a]=0$).

  Next, we prove the theorem. Let $V_0 = \{v\in V|\a
  v=0\}$, which is non-zero by the inductive hypothesis. We claim that $\g
  V_0\subseteq V_0$. To see this, take $x\in \g$, $v\in V_0$, and $y\in \a$. We have
  to check that $y(xv)=0$. But
  \[yxv = x\underbrace{yv}_0+\underbrace{[y,x]}_{\in \a}v=0.\]
  Now, we have that $\g=kX\oplus \a$, and $\a$ kills $V_0$, and that
  $X:V_0\to V_0$ is nilpotent, so it has a kernel. Thus, there is some $v\in
  V_0$ which is killed by $X$, and so $v$ is killed by all of $\g$.
 \end{proof}
 \begin{corollary}\label{lec11Engelcor2}
   If $ad_X$ is nilpotent for every $X\in \g$, then $\g$ is nilpotent as a Lie algebra.
 \end{corollary}
 \begin{proof}
   Let $V=\g$, so we have $ad:\g\to \gl(\g)$, which has kernel $Z(\g)$. By Engel's
   theorem, we know that there is an $x\in\g$ such that $(ad\,\g)(x)=0$. This implies that
   $Z(\g)\neq 0$. By induction we can assume $\g/Z(\g)$ is nilpotent. But then $\g$
   itself must be nilpotent as well because $\D_n(\g/Z(\g))=0$  implies
   $\D_{n+1}(\g)=0$.
 \end{proof}

 \begin{warning} If $\g\subseteq \gl(V)$ is a nilpotent subalgebra, it does not
 imply that every $X\in \g$ is nilpotent (take diagonal matrices for example).
 \end{warning}
 \begin{theorem}[Lie's Theorem]\label{lec11Lie} \index{Lie's Theorem|idxbf}
   Let $k$ be algebraically closed and of characteristic 0. If $\g\subseteq \gl(V)$ is a
   solvable subalgebra, then all elements of $\g$ have a common eigenvector in $V$.
 \end{theorem}
 This is a generalization of the statement that two commuting operators have a common
 eigenvector.
 \begin{corollary}\label{lec11Liecor}
   If $\g$ is solvable, then there is a complete flag
 $\{0\}=V_0\varsubsetneq V_1\varsubsetneq \cdots \varsubsetneq V_n=V$
 such that $\g(V_i)\subseteq V_i$. That is, there is a basis in which all elements of
 $\g$ are upper triangular.
 \end{corollary}
 \begin{proof}
   If $\g$ is solvable, take any subspace $\a\subset \g$ of codimension 1 containing
   $\D\g$, then $\a$ is an ideal. We're going to try to do the same kind of induction
   as in Engel's theorem.

   For a linear functional $\lambda: \a\to k$, let
   \[
    V_\lambda=\{v\in V| Xv = \lambda(X)v \text{ for all } X\in \a\}.
   \]
   $V_\lambda\neq 0$ for some $\lambda$ by induction hypothesis.
 \begin{claim}
   $\g(V_\lambda)\subseteq V_\lambda$.
 \end{claim}
 \begin{proof}[Proof of Claim]
  Choose $v\in V_\lambda$ and $X\in \a$, $Y\in \g$. Then
  \begin{align*}
    X(Yv) &= Y(\underbrace{Xv}_{\lambda(X)v}) +
    \underbrace{[X,Y]v}_{\lambda([X,Y])v}
  \end{align*}
  We want to show that $\lambda[X,Y]=0$. There is a trick. Let $r$ be the largest
  integer such that $v, Yv, Y^2v, \dots, Y^r v$ is a linearly independent set. We know
  that $Xv=\lambda(X)v$ for any $X\in \a$. We claim that $XY^jv \equiv \lambda(X)Y^jv
  \mod (span\{v,Yv,\ldots,Y^{j-1}v\})$. This is clear for $j=0$, and by induction, we
  have
  \begin{align*}
    X Y^j v & = Y\hspace{-1em}\underbrace{XY^{j-1}v}_{
                \substack{\equiv \lambda(X)Y^{j-1}v \\
                \hspace{-3.25em} \mod span\{v,\dots, Y^{j-2}v\}}}
                +\hspace{-2em} \underbrace{[X,Y]Y^{j-1}v}_{%
                    \substack{\equiv \lambda([X,Y])Y^{j-1}v \\ \hspace{1.75em}
                    \mod span\{v,\dots, Y^{j-2}v\}}}\\
      &\equiv \lambda(X) Y^j v \mod span\{v,\dots, Y^{j-1}v\}
  \end{align*}
%  \footnote[1]{Notes editor's comment: Use
%  $XY^jv=YXY^{j-1}v+[X,Y]Y^{j-1}v$ and induction on $j$. Warning: You can't argue by
%  saying $XY^jv=Y^jXv+[X,Y^j]v=\lambda(X)Y^jv+\lambda([X,Y^j])v$, because $Y^j$ might
%  not be in $\g$ and therefor $[X,Y^j]$ might not be in $\a$.}.

  So the matrix for $X$ can be written as $\lambda(X)$ on the diagonal and stuff above
  the diagonal (in this basis). So the trace of $X$ is $(r+1)\lambda(X)$. Then we have
  that $tr([X,Y])=(r+1)\lambda([X,Y])$, since the above statement was proved for any
  $X\in \a$ and $[X,Y]\in\a$. But the trace of a commutator is always 0. Since the
  characteristic of $k$ is 0, we can conclude that $\lambda[X,Y]=0$.
 \renewcommand{\qedsymbol}{$\square_{\text{Claim}}$}
 \end{proof}
 To finish the proof, write $\g=kT\oplus \a$, with $T:V_{\lambda}\to V_\lambda$ (we
 can do this because of the claim). Since $k$ is algebraically closed, $T$ has a
 non-zero eigenvector $w$ in $V_\lambda$. This $w$ is the desired common eigenvector.
 \end{proof}
 \begin{remark}
   If $k$ is not algebraically closed, the theorem doesn't hold. For example, consider
   the (one dimensional) Lie algebra generated by a rotation of $\RR^2$.

   The theorem also fails if $k$ is not characteristic 0. Say $k$ is characteristic
   $p$, then let
%   \[
%    x = \mat{0 & 1 &  & & 0\\
%               & 0 & 1 & & \\
%               0&   &  & \ddots & 0 \\
%               1 & 0 & & & 0 } \quad , \quad
%    y = \mat{0 & & & 0\\
%               &1& & \\
%               & &2 &\\
%               & & &\ddots\\
%               & & & & p-1}
%   \]
   $x$ be the permutation matrix of the $p$-cycle $(p\ p-1\ \cdots \ 2\ 1)$ (i.e.\ the
   matrix $\matrix 0{I_{p-1}}10$), and let $y$ be the diagonal matrix
   $diag(0,1,2,\dots, p-1)$. Then $[x,y]=x$, so the Lie algebra generated by $x$ and
   $y$ is solvable. However, $y$ is diagonal, so we know all of its eigenvectors, and
   none of them is an eigenvector of $x$.
 \end{remark}

 \begin{corollary}
   Let $k$ be of characteristic 0. Then $\g$ is solvable if and only if $\D\g$ is
   nilpotent.
 \end{corollary}
 If $\D\g$ is nilpotent, then $\g$ is solvable from the definitions. If $\g$ is
 solvable, then look at everything over the algebraic closure of $k$, where $\g$ looks
 like upper triangular matrices, so $\D\g$ is nilpotent. All this is independent of
 coefficients (by useful fact (\ref{lec11Fscalers})).

 \subsektion{The radical}

 There is a unique maximal solvable ideal in $\g$ (by useful fact
(\ref{lec11Fsumsolv}): sum of solvable ideals is solvable), which is called the
radical of $\g$.
 \begin{definition}\label{lec11def:semisimple}
   We call $\g$ \emph{semisimple}\index{semisimple} if $\mathrm{rad}\,\g=0$.
 \end{definition}
 \begin{exercise}
   Show that $\g/\mathrm{rad}\,\g$ is always semisimple.
   \begin{solution}
     Let $\pi:\g\to \g/\mathrm{rad}\,\g$ be the canonical projection, and assume
     $\a\in \g/\mathrm{rad}\,\g$ is solvable. Then $\D^k \a=0$ for some $k$, so $\D^k
     \pi^{-1}(\a)\subseteq \mathrm{rad}\,\g$. Since $\mathrm{rad}\,\g$ is solvable, we
     have that $\D^N \pi^{-1}(\a)=0$ for some $N$. By definition of
     $\mathrm{rad}\,\g$, we get that $\pi^{-1}(\a)\subseteq \mathrm{rad}\,\g$, so
     $\a=0\subseteq \g/\mathrm{rad}\,\g$. Thus, $\g/\mathrm{rad}\,\g$ is semisimple.
   \end{solution}
 \end{exercise}
 If $\g$ is one dimensional, generated by $X$, then we have that $[\g,\g]=0$, so $\g$
 cannot be semisimple.

 If $\g$ is two dimensional, generated by $X$ and $Y$, then we have that $[\g,\g]$ is
 one dimensional, spanned by $[X,Y]$. Thus, $\g$ cannot be semisimple because $\D\g$
 is a solvable ideal.

 There is a semisimple Lie algebra of dimension $3$, namely $\sl_2$.

 Semisimple algebras have really nice properties. Cartan's criterion (Theorem
 \ref{lec12Cartan}) says that $\g$ is semisimple if and only if the Killing form (see
 next lecture) is non-degenerate. Whitehead's theorem (Theorem \ref{lec12Whitehead})
 says that if $V$ is a non-trivial irreducible representation of a semisimple Lie
 algebra $\g$, then $H^i(\g,V)=0$ for all $i$. Weyl's theorem (Theorem
 \ref{lec12Weyl}) says that every finite dimensional representation of a semisimple
 Lie algebra is the direct sum of irreducible representations. If $G$ is simply
 connected and compact, then $\g$ is semisimple (See Lecture 20).
