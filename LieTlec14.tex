 \stepcounter{lecture}
 \setcounter{lecture}{14}
 \sektion{Lecture 14 - More on Root Systems}


% Vera Serganova's office hours are 5-6:30 TuTh, by the way.

% Today we will keep talking about root systems.

 Assume $\g$ is semisimple. Last time, we started with a regular element $h\in
 \g_{ss}$ and constructed the decomposition $\g = \h\oplus \bigoplus_{\alpha \in
 \Delta} \g_\alpha$, where $\Delta\subseteq \h^*$ is the set of roots. We proved that
 each $\g_\alpha$ is one dimensional (we do not call 0 a root). For each root, we
 associated an $\sl(2)$ subalgebra. Given $X_\alpha \in \g_\alpha, Y_\alpha\in
 \g_{-\alpha}$, we set $H_\alpha = [X_\alpha,Y_\alpha]$, and normalized so that
 $\alpha(H_\alpha)=2$.

 Furthermore, we showed that
 \begin{enumerate}
 \item $\Delta$ spans $\h^*$,
 \item $\alpha (H_\beta)\in \ZZ$, with $\alpha
 - \alpha(H_\beta)\beta \in \Delta$ for all $\alpha, \beta \in \Delta$, and
 \item if $\alpha, k\alpha \in \Delta$, then $k=\pm 1$.
 \end{enumerate}

 How unique is this decomposition? We started with some choice of a regular semisimple
 element. Maybe a different one would have produced a different Cartan subalgebra.

 \begin{theorem}\label{lec14T:CSA}
   Let $\h$ and $\h'$ be two Cartan subalgebras\index{Cartan!subalgebra} of a
   semisimple Lie algebra $\g$ (over an algebraically closed field of characteristic
   zero). Then there is some $\phi\in Ad\, G$ such that $\phi(\h)=\h'$. Here, $G$ is
   the Lie group associated to $\g$.
 \end{theorem}
 \begin{proof}
   Consider the map
   \begin{align*}
     \Phi:\h^{\text{reg}}\times \g_{\alpha_1}\times \cdots \times \g_{\alpha_N} &\to \g \\
     (h,x_1,\dots, x_N)\qquad & \mapsto \exp(ad_{x_1})\cdots \exp(ad_{x_N}) h.
   \end{align*}
   Note that $ad_{x_i}h$ is linear in both $x_i$ and $h$, and each $ad_{\g_{\alpha_i}}$
   is nilpotent, so the power series for $\exp$ is finite. It follows that $\Phi$ is a
   polynomial function. Since $\der{}{t}\exp(ad_{tx_i})h\big|_{t=0}=\alpha_i(h)x_i\in
   \g_{\alpha_i}$, the differential if $\Phi$ at $(h,0,\dots,0)$ is
   \[
   D\Phi|_{(h,0,\dots, 0)} =
     \left(\begin{array}{c|ccc}
     \id_\h & & 0 \\ \hline
      & \alpha_1(h) & & \raisebox{-1ex}{\smash{\llap{\LARGE $0$}}}\\
     0& & \ddots\\
      & \smash{\mbox{\LARGE $0$}} & & \alpha_N(h)
     \end{array}\right)
   \]
   with respect to the decomposition $\g=\h\oplus \g_{\alpha_1}\oplus\cdots\oplus
   \g_{\alpha_N}$. $D\Phi|_{(h,0,\dots, 0)}$ is non-degenerate because $h\in
   \h^{\text{reg}}$ implies that $\alpha_i(h)\neq 0$. So $\im \Phi$ contains a Zariski
   open set\index{Zariski open set}.\footnote{This is a theorem from algebraic geometry.
   \cite{FulHar} claims in \S D.3 that this result is in \cite{Hartshorne}, but I
   cannot find it.} Let $\Phi'$ be the analogous map for $\h'$. Since Zariski open sets
   are dense, we have that $\im \Phi\cap \im \Phi' \neq \varnothing$. So there are
   $\psi,\psi'\in Ad\, G$, and $h\in \h, h'\in \h'$ such that $\psi(h)=\psi'(h')$. Thus,
   we have that $\h=\psi^{-1}\psi'(\h)$.
 \end{proof}

 \subsektion{Abstract Root systems}
 We'd like to forget that any of this came from a Lie algebra. Let's just study an
 abstract set of vectors in $\h^*$ satisfying some properties. We know that $B$ is
 non-degenerate on $\h$, so there is an induced isomorphism $s:\h\to \h^*$. By
 definition, $\langle s(h),h'\rangle = B(h,h')$.

 Let's calculate
 \begin{align*}
   \langle sH_\beta, H_\alpha\rangle &= B(H_\beta,H_\alpha) = B(H_\alpha,H_\beta) & \text{($B$ symmetric)}\\
    &= B(H_\alpha, [X_\beta, Y_\beta]) = B([H_\alpha, X_\beta], Y_\beta) &\text{($B$ invariant)} \\
    &=  B(X_\beta,Y_\beta) \beta(H_\alpha) \\
    &= \frac{1}{2} B([H_\beta,X_\beta],Y_\beta) \beta(H_\alpha) & (2X_\beta = [H_\beta,X_\beta])\\
    &=  \frac{1}{2} B(H_\beta, H_\beta)\beta(H_\alpha) & \text{($B$ invariant)}
 \end{align*}
 Thus, we have that $s(H_\beta) = \frac{B(H_\beta,H_\beta)}{2} \beta$. Also, compute
 \begin{align}
   (\alpha, \beta) &:= \langle \alpha, s^{-1}\beta\rangle\notag\\
   &= \alpha\left( \frac{2H_\beta}{B(H_\beta,H_\beta)}\right)\notag\\
   &= \frac{2\alpha(H_\beta)}{B(H_\beta,H_\beta)}. \label{lec14ddag}
 \end{align}
 In particular, letting $\alpha=\beta$, we get $s(H_\beta) =
 \frac{2\beta}{(\beta,\beta)}$. This is sometimes called the
 \emph{coroot}\index{coroot} of $\beta$, and denoted $\check\beta$. We may use
 (\ref{lec14ddag}) to rewrite fact \ref{lec13p2} from last time as:
 \[
 \text{For $\alpha,\beta\in \Delta$, $\frac{2(\alpha,\beta)}{(\beta,\beta)}\in \ZZ$, and
 $\alpha - \frac{2(\alpha,\beta)}{(\beta,\beta)}\beta\in \Delta$.}\tag{$2'$}
 \]
 Now you can define $r_\beta:\h^*\to \h^*$ by $r_\beta(x) =
 x-\frac{2(x,\beta)}{(\beta,\beta)}\beta$. This is the reflection through the
 hyperplane orthogonal to $\beta$ in $\h^*$. The group generated by the $r_\beta$ for
 $\beta\in \Delta$ is a Coxeter group\index{Coxeter group}. If we want to study
 Coxeter groups, we'd better classify root systems.\footnote{We will not talk about
 Coxeter groups in depth in this class.}

 We want to be working in Euclidean space, but we are now in $\h^*$. Let $\h_r$ be the
 real span\footnote{Assuming we are working over $\CC$. Otherwise, we can use the
 $\QQ$ span.} of the $H_\alpha$'s. We claim that $B$ is positive definite on $\h_r$.
 To see this, note that $X_\alpha, Y_\alpha, H_\alpha$ make a little $\sl(2)$ in $\g$,
 and that $\g$ is therefore a representation of $\sl(2)$ via the adjoint actions
 $ad_{X_\alpha},ad_{Y_\alpha},ad_{H_\alpha}$. But we know that in any representation
 of $\sl(2)$, the eigenvalues of $H_\alpha$ must be integers. so
 $ad_{H_\alpha}\circ ad_{H_\alpha}$ has only positive eigenvalues, so
 $B(H_\alpha,H_\alpha)=tr(ad_{H_\alpha}\circ ad_{H_\alpha})>0$.

 Thus, we may think of our root systems in Euclidean space, where the inner
 product on $\h^*$ is given by $(\mu,\nu)\stackrel{def}{=} B(s^{-1}(\mu),s^{-1}(\nu))
 = \langle \mu,s^{-1}\nu\rangle$.

 \begin{definition}
   An \emph{abstract reduced root system}\index{root system!abstract} is a finite set
   $\Delta\subseteq \RR^n\smallsetminus\{0\}$ which satisfies
   \begin{itemize}
   \item[\hypertarget{RS1}{(RS1)}] $\Delta$ spans $\RR^n$,%
   \item[\hypertarget{RS2}{(RS2)}] if $\alpha,\beta\in \Delta$, then
    $\frac{2(\alpha,\beta)}{(\beta,\beta)}\in \ZZ$, and $r_\beta(\Delta)=\Delta$%
   \item[](i.e.\ $\alpha,\beta\in \Delta \Rightarrow r_\beta(\alpha)\in \Delta$, with
   $\alpha-r_\beta(\alpha) \in \ZZ \beta$ ), and %
   \item[\hypertarget{RS3}{(RS3)}] if $\alpha,k\alpha\in \Delta$, then $k=\pm 1$ (this
        is the ``reduced'' part).%
   \end{itemize}
   The number $n$ is called the \emph{rank}\index{rank} of $\Delta$.
 \end{definition}
 Notice that given root systems $\Delta_1\subset \RR^n$, and $\Delta_2\subset \RR^m$, we
 get that $\Delta_1\coprod \Delta_2 \subset \RR^n\oplus \RR^m$ is a root system.
 \begin{definition}
   A root system is \emph{irreducible}\index{root system!irreducible} if it cannot be
   decomposed into the union of two root systems of smaller rank.
 \end{definition}
 \begin{exercise}
   Let $\g$ be a semisimple Lie algebra and let $\Delta$ be its root system. Show that
   $\Delta$ is irreducible if and only if $\g$ is simple.
   \begin{solution}
     If $\Delta$ is reducible, with $\Delta=\Delta_1\cup \Delta_2$, then set $\h_i^*$ to
     be the span of $\Delta_i$, and set $\g_i=\h_i\oplus \bigoplus_{\alpha\in
     \Delta_i}\g_\alpha$ (for $i=1,2$). Then we have that $\g=\g_1\oplus \g_2$ as a
     vector space. We must check that $\g_1$ is an ideal (the by symmetry, $\g_2$ will
     also be an ideal). From the relation $[\g_\alpha,\g_\beta]\subseteq
     \g_{\alpha+\beta}$, we know that it is enough to check that for $\alpha\in
     \Delta_1$,
     \begin{align*}
     [\g_\alpha,\g_{-\alpha}]&\subseteq \h_1, \text{ and} \tag{1}\\
     [\g_\alpha,\h_2]&=0.\tag{2}
     \end{align*}
     Letting $\beta\in \Delta_2$, we have that
     $\beta([X_\alpha,Y_\alpha])=\beta(H_\alpha) =
     \frac{2(\alpha,\beta)}{(\beta,\beta)}=0$ because $\Delta_1$ and $\Delta_2$ are
     orthogonal; 1 follows because $\Delta_2$ spans the orthogonal complement of
     $\h_1$ in $\h$. Similarly, we have
     $[X_\alpha,H_\beta]=\alpha(H_\beta)X_\alpha=0$; 2 follows because the
     $H_\beta$ span $\h_2$.

     Conversely, if $\g=\g_1\oplus \g_2$ as a Lie algebra, then take root
     decompositions $\g_1=\h_1\oplus \bigoplus_{\alpha\in \Delta_1}\g_\alpha$ and
     $\g_2=\h_2\oplus \bigoplus_{\beta\in \Delta_2} \g_\beta$, with respect to regular
     elements $h_1\in \h_1$ and $h_2\in \h_2$. Then for $x_1\in\g_1$ and $x_2\in
     \g_2$, we have that $[h_1+h_2,x_1+x_2]=[h_1,x_1]+[h_2,x_2]$; it follows that
     $h_1+h_2$ is a regular element in $\g$. The Cartan given by this element is
     clearly $\h_1\oplus \h_2$. If $x\in \g_\alpha\subseteq \g_1$, then we have
     $[h_1+h_2,x]=\alpha(h_1)x+0$, so $\alpha$ is a root. Similarly, each $\beta\in
     \Delta_2$ is a root. Since we have accounted for all the root spaces of $\g_1$
     and of $\g_2$, we have a root decomposition $\g=(\h_1\oplus \h_2)\oplus
     \bigoplus_{\alpha\in \Delta_1}\g_\alpha \oplus \bigoplus_{\beta\in
     \Delta_2}\g_\beta$. This shows that $\Delta=\Delta_1\cup \Delta_2$.
   \end{solution}
 \end{exercise}

 Now we will classify all systems of rank 2. Observe that
 $\frac{2(\alpha,\beta)}{(\alpha,\alpha)}\frac{2(\alpha,\beta)}{(\beta,\beta)} =
 4\cos^2 \theta$, where $\theta$ is the angle between $\alpha$ and
 $\beta$. This thing must be an integer. Thus, there are not many choices for
 $\theta$:
 \[\begin{array}{c|ccccc}
   \cos \theta & 0 & \pm \frac{1}{2} & \pm \frac{1}{\sqrt{2}} & \pm
   \frac{\sqrt{3}}{2}\\ \hline
   \theta & \frac{\pi}{2} & \frac{\pi}{3}, \frac{2\pi}{3} & \frac{\pi}{4}, \frac{3\pi}{4}
   & \frac{\pi}{6},\frac{5\pi}{6}
 \end{array}\]
 Choose two vectors with minimal angle between them. If the minimal angle is $\pi/2$,
 then the system is reducible.
 \[\begin{xy}
   <1.5em,0em>:(0,0);
   \ar (1.4,0) *+!L{\alpha},
   \ar (-1.4,0),
   \ar (0,1) *+!D{\beta},
   \ar (0,-1),
 \end{xy}\]
 Notice that $\alpha$ and $\beta$ can be scaled independently.

 If the minimal angle is smaller than $\pi/2$, then $r_\beta(\alpha)\neq \alpha$, so
 the difference $\alpha - r_\beta(\alpha)$ is a non-zero integer multiple of $\beta$
 (in fact, a positive multiple of $\beta$ since $\theta < \pi/2$). If we assume
 $\|\alpha\|\le \|\beta\|$ (we can always switch them), we get that $\|\alpha -
 r_\beta(\alpha)\|<2\|\alpha\|\le 2\|\beta\|$. It follows that
 $\alpha-r_\beta(\alpha)=\beta$.
 \begin{remark}\label{lec14R:acute}
   Observe that we have shown that for any roots $\alpha$ and $\beta$, if
   $\theta_{\alpha,\beta}<\pi/2$, then $\alpha-\beta$ is a root.
 \end{remark}
 \begin{remark}\label{lec14Rmkrank2}
   We have also shown that once we set the direction of the longer root, $\beta$ (thus
   determining $r_\beta$), its length is determined relative to the length of
   $\alpha$.
 \end{remark}
 Now we can obtain the remaining elements of the root system from the
 condition that $\Delta$ is invariant under $r_\alpha$ and $r_\beta$, observing that
 no additional vectors can be added without violating \hyperlink{RS2}{RS2},
 \hyperlink{RS3}{RS3}, or the prescribed minimal angle. Thus, all the irreducible rank
 two root systems are
 \[\begin{array}{ccccc}
   A_2, \theta = \pi/3 &\quad & B_2, \theta = \pi/4 &\quad & G_2, \theta=\pi/6\\
   \begin{xy}
   <3em,0em>:(0,0)="c";
   a(150);a(-30) **@{.},
   \ar "c";a(0) *+!L{\alpha}
   \ar "c";a(60) *+!L{\beta}
   \ar "c";a(120)
   \ar "c";a(180)
   \ar "c";a(-60) *+!U{r_\beta(\alpha)}
   \ar "c";a(-120)
 \end{xy} & &
 \begin{xy}
   (-1.3,1.3);(-1,1) **@{.},
   (1.3,-1.3);(1,-1) **@{.};
   (0,0);(0,0);
   \ar (1,0) *+!L{\alpha},
   \ar (1,1) *+!L{\beta},
   \ar (0,1),
   \ar (-1,1),
   \ar (-1,0),
   \ar (-1,-1),
   \ar (0,-1) *+!U{r_\beta (\alpha)},
   \ar (1,-1),
 \end{xy} & &
 \begin{xy}
   <2em,0em>:
   a(120)+a(120);a(120) **@{.},
   a(-60)+a(-60);a(-60) **@{.},
   (0,0);(0,0);
   \ar (1,0) ="a" *+!L{\alpha},
   \ar a(120) -"a"="b",
   \ar "b"+"a",
   \ar "b"+"a"+"a",
   \ar "b"+"a"+"a"+"a" *+!L{\beta},
   \ar "b"+"b"+"a"+"a"+"a",
   \ar -"a",
   \ar -"b",
   \ar -"b"-"a",
   \ar -"b"-"a"-"a" *+!U{r_\beta (\alpha)\,},
   \ar -"b"-"a"-"a"-"a",
   \ar -"b"-"b"-"a"-"a"-"a",
 \end{xy}
 \end{array}\]

 \subsektion{The Weyl group}\index{Weyl group|(idxbf} Given a root system $\Delta =
 \{\alpha_1,\dots, \alpha_N\}$, we call the group generated by the $r_{\alpha_i}$s the
 \emph{Weyl group}, denoted $\weyl$.

 \begin{remark}
   If $G$ is a Lie group with Lie algebra $\g$, then for each $r_\alpha\in \weyl$, there
   is a group element $S_\alpha\in G$, such that $Ad_{S_\alpha}$ takes $\h$ to itself,
   and induces $r_\alpha$. Consider the $\sl_2\subseteq \g$ generated
   by $X_\alpha$, $Y_\alpha$, and $H_\alpha$. The embedding $\sl_2\to \g$ induces a
   homomorphism $SL(2)\to G$, and $S_\alpha$ is the image of $\matrix 01{-1}0$ under
   this homomorphism.
 \end{remark}
 \begin{exercise}\label{lec14Ex:Weyl}
   Let $\g$ be a semisimple Lie algebra, and let $\h$ be a Cartan subalgebra. For each
   root $\alpha$ define
   \[
    S_\alpha = \exp(X_\alpha)\exp(-Y_\alpha)\exp(X_\alpha).
   \]
   Prove that $Ad_{S_\alpha}(\h)=\h$ and that
   \[
    \langle \lambda, Ad_{S_\alpha}(h)\rangle = \langle r_\alpha(\lambda),h\rangle
   \]
   for any $h\in \h$ and $\lambda\in \h^*$, where $r_\alpha$ is the reflection in
   $\alpha^\perp$.
   \begin{solution} \label{lec14Soln:Weyl}
     Note that
     $Ad_{S_\alpha}=\exp(ad_{X_\alpha})\exp(-ad_{Y_\alpha})\exp(ad_{X_\alpha})$. If
     $h\in \h$, then $ad_{X_\alpha}h=-\alpha(h)X_\alpha$ and
     $ad_{X_\alpha}ad_{X_\alpha}(h)=\alpha(h)ad_{X_\alpha}(X_\alpha)=0$. Using the
     power series expansion for $\exp$, we get that
     \[
       \exp(ad_{X_\alpha})(h)=h - \alpha(h)X_\alpha.
     \]
     Similarly, we apply $\exp(-ad_{Y_\alpha})$ to the result
     \begin{align*}
       \exp(-ad_{Y_\alpha})\bigl(h-&\alpha(h)X_\alpha\bigr) \\
        &= h-\alpha(h)Y_\alpha -
       \alpha(h)\bigl( X_\alpha -\underbrace{[Y_\alpha,X_\alpha]}_{-H_\alpha} +
       \underbrace{\half[Y_\alpha,[Y_\alpha,X_\alpha]]}_{-\half\alpha(H_\alpha)Y_\alpha=-Y_\alpha}
       + 0 \bigr)\\
       &= h - \alpha(h)(X_\alpha + H_\alpha)
     \end{align*}
     and then apply $\exp(ad_{X_\alpha})$
     \begin{align*}
       \exp(ad_{X_\alpha})\bigl(h-&\alpha(h)(X_\alpha+H_\alpha)\bigr)\\
           &= h-\alpha(h)X_\alpha - \alpha(h)\Bigl(\bigl( X_\alpha + 0
           \bigr)+\bigl( H_\alpha - \alpha(H_\alpha)X_\alpha + 0 \bigr)\Bigr)\\
           &= h - \alpha(h)H_\alpha.
     \end{align*}
     This shows that $Ad_{S_\alpha}(\h)=\h$. For $\lambda\in \h^*$, we get
     \begin{align*}
       \langle r_\alpha(\lambda),h\rangle &=
       \lambda(h)-\frac{2(\lambda,\alpha)}{(\alpha,\alpha)}\alpha(h)\\
       &= \lambda(h) - \frac{2\lambda(H_\alpha)}{\alpha(H_\alpha)}\alpha(h) &
       \text{(using Equation \ref{lec14ddag})}\\
       &= \lambda\bigl( h - \alpha(h)H_\alpha \bigr) & \bigl( \alpha(H_\alpha)=2 \bigr)\\
       &= \langle \lambda, Ad_{S_\alpha}(h)\rangle.
     \end{align*}
   \end{solution}
 \end{exercise}
 If $G$ is a connected group with Lie algebra $\g$, then define the \emph{Cartan
 subgroup}\index{Cartan!subgroup} $H\subseteq G$ to be the subgroup generated by the
 image of $\h$ under the exponential map $\exp:\g\to G$. Let
 \[
    N(H)=\{g\in G|gHg^{-1}=H\}
 \]
 be the normalizer of $H$. Then we get a sequence of homomorphisms
 \begin{align*}
    N(H)\to \aut H \to &\aut \h \to \aut \h^*\\
    g\xmapsto{\qquad\qquad\qquad} &\ Ad_g \longmapsto Ad^*_g.
 \end{align*}
 The first map is given by conjugation, the second by differentiation at the identity,
 and the third by the identification of $\h$ with $\h^*$ via the Killing form. The
 final map is given by $g\mapsto Ad^*_g$, where
 $\big(Ad^*_g(l)\big)(h)=l(Ad_{g^{-1}}h)$.
 \begin{proposition}\label{lec14P:WeylgpFact}
   The kernel of the composition above is exactly $H$, and the image is the Weyl
   group. In particular, $\weyl\cong N(H)/H$.
 \end{proposition}
 Before we prove this proposition, we need a lemma.
 \begin{lemma}\label{lec14L:H=C(H)}
   The centralizer of $H$ is $H$.
 \end{lemma}
 \begin{proof}
   If $g$ centralizes $H$, then $Ad_g$ is the identity on $\h$. Furthermore, for any
   $h\in \h$ and $x\in \g_\alpha$,
   \[
     [h,Ad_g x] = Ad_g([h,x])=Ad_g (\alpha(h) x) = \alpha(\h)\cdot Ad_g x
   \]
   so $Ad_g(\g_\alpha)=\g_\alpha$. Say $Ad_g(X_i)=c_iX_i$, where $X_i$ spans the
   simple root space $\g_{\alpha_i}$. Then $Ad_g(Y_i)=\frac{1}{c}Y_i$. Since the
   simple roots are linearly independent, we can find an $h\in \h$ such that $Ad_{\exp
   h}X_i=c_iX_i$. Now we have that $Ad_{g\cdot (\exp h)^{-1}}$ is the identity on
   $\g$, so $g\cdot (\exp h)^{-1}$ is in the center of $G$, which is in $H$\anton{ugg,
   why is $Z(G)\subseteq H$?}, so $g\in H$, as desired.
 \end{proof}
 \begin{proof}[Proof of Proposition \ref{lec14P:WeylgpFact}]
   It is clear that $H$ is in the kernel of the composition. To see that $H$ is
   exactly the kernel, observe that $Ad^*_g$ can only be the identity map if $Ad_g$ is
   the identity map, which can only happen if conjugation by $g$ is the identity map
   on $H$, i.e.\ if $g$ is in the centralizer of $H$. By Lemma \ref{lec14L:H=C(H)},
   $g\in H$.

   Since the $S_\alpha$ in the previous exercise preserves $\h$ under the $Ad$ action,
   it is in the normalizer of $H$. It is easy to see (given Exercise \ref{lec14Ex:Weyl})
   that the image of $S_\alpha$ in $\aut \h^*$ is exactly $r_\alpha$. Thus, every
   element of the Weyl group is in the image.

   We can show that the map preserves the set of roots. If $\alpha$ is a root, with a
   root vector $x$, then we have $ad_h(x)=\alpha(h)x$ for all $h\in \h$. We would like
   to show that $Ad^*_g \alpha$ is also a root. It is enough to observe that $Ad_{g}x$
   is a root vector:
   \begin{align*}
     ad_h(Ad_{g}x) &= [h,Ad_{g}x] = Ad_{g}\big( [Ad_{g^{-1}} h, x] \big)\\
        &= Ad_{g}\big( \alpha(Ad_{g^{-1}} h) x) \big) = \alpha(Ad_{g^{-1}}h) Ad_g(x)\\
        &= \big(Ad^*_g(\alpha)\big)(h)\, (Ad_g x)
   \end{align*}

   Therefore, we can find some element $w$ in the Weyl group so that $w\circ Ad^*_g$
   preserves the set $\Pi$ of simple roots. Since $w$ is in the image of $Ad^*$, it is
   enough to show that whenever $Ad^*_g$ preserves $\Pi$, it is the identity map on
   $\h^*$.

   \anton{complete this proof ... is there a way that doesn't use representation
   theory. For any dominant integral weight $\lambda$,  $V_\lambda$ is isomorphic to
   $V_{Ad^*_g \lambda}$ via $\rho(g)$, so $\lambda=Ad^*_g\lambda$ by Theorem
   \ref{lec18Thm:hiweight}. It follows that $Ad^*_g$ is the identity on $\h^*$.}
 \end{proof}

 \begin{example}[Also see Example \ref{lec13Eg:sl3}]\label{lec14Eg:A_n}
   The root system of $\sl_{n+1}$ is called $A_n$\index{An@$A_n$!and $\sl_{n+1}$|idxbf}.
   We pick an orthonormal basis $\varepsilon_1,\dots,\varepsilon_{n+1}$ of
   $\RR^{n+1}$, the the root system is the set of all the differences: $\Delta = \{
   \varepsilon_i - \varepsilon_j | i\neq j\}$. We have that
   \[
    r_{\varepsilon_i - \varepsilon_j}(\varepsilon_k) = \left\{
    \begin{array}{cl}
      \varepsilon_k & k\neq i,j\\
      \varepsilon_j & k=i\\
      \varepsilon_i & k=j
    \end{array} \right.
   \]
   is a transposition, so we have that $\weyl\simeq S_{n+1}$.
 \end{example}

 Now back to classification of abstract root systems.

 Draw a hyperplane in general position (so that it doesn't contain any roots). This
 divides $\Delta$ into two parts, $\Delta = \Delta^+\coprod \Delta^-$. The roots in
 $\Delta^+$ are called \emph{positive roots}\index{root!positive}, and the elements of
 $\Delta^-$ are called negative roots. We say that $\alpha\in \Delta^+$ is
 \emph{simple}\index{root!simple} if it cannot be written as the sum of other positive
 roots. Let $\Pi$ be the set of simple roots, sometimes called a base. It has the
 properties\index{root!simple!properties of|(idxbf}
 \begin{enumerate}
 \item \label{lec14n1} Any $\alpha\in \Delta^+$ is a sum of simple roots (perhaps with
 repitition): $\alpha = \sum_{\beta \in \Pi} m_\beta \beta$ where $m_\beta \in
 \ZZ_{\ge 0}$.

 \item \label{lec14n2} If $\alpha, \beta\in \Pi$, then $(\alpha,\beta)\le 0$.

 This follows from the fact that if $(\alpha,\beta)>0$, then $\alpha-\beta$ and
 $\beta-\alpha$ are again roots (as we showed when we classified rank 2 root systems),
 and one of them is positive, say $\alpha-\beta$. Then $\alpha = \beta +
 (\alpha-\beta)$, contradicting simplicity of $\alpha$.

 \item \label{lec14n3} $\Pi$ is a linearly independent set.

 If they were linearly dependent, the relation $\sum_{\alpha_i\in \Pi} a_i\alpha_i =
 0$ must have some negative coefficients (because all of $\Pi$ is in one half space),
 so we can always write
 \[
    0\neq a_1\alpha_1+\cdots+ a_r\alpha_r = a_{r+1}\alpha_{r+1}+\cdots+ a_n\alpha_n
 \]
 with all the $a_i\ge 0$. Taking the inner product with the left hand side, we get
 \begin{align*}
    \|a_1\alpha_1+&\cdots+ a_r\alpha_r\|^2 \\
                &= (a_1\alpha_1+\cdots+ a_r\alpha_r, a_{r+1}\alpha_{r+1}+\cdots+ a_n\alpha_n) \le 0
 \end{align*}
 by \ref{lec14n2}, which is absurd.
 \end{enumerate}\index{root!simple!properties of|)idxbf}
 \begin{remark}
   Notice that the hyperplane is $t^\perp$ for some $t$, and the positive roots are the
   $\alpha \in \Delta$ for which $(t,\alpha)>0$. This gives an order on the roots. You
   can inductively prove \ref{lec14n1} using this order.
 \end{remark}
 \begin{remark}
   Notice that when you talk about two roots, they always generate one of the rank 2
   systems, and we know what all the rank 2 systems are.
 \end{remark}

 \begin{lemma}[Key Lemma]\label{lec14L:key}
 Suppose we have chosen a set of positive roots $\Delta^+$, with simple roots $\Pi$.
 Then for $\alpha\in \Pi$, we have that $r_\alpha(\Delta^+)= \Delta^+ \cup \{-\alpha\}
 \smallsetminus \{\alpha\}$.
 \end{lemma}
 \begin{proof}
 For a simple root $\beta\neq \alpha$, we have $r_\alpha(\beta) = \beta + k\alpha$ for
 some non-negative $k$; this must be on the positive side of the hyperplane, so it is
 a positive root. Now assume you have a positive root of the form $\gamma = m\alpha +
 \sum_{\alpha_i\neq \alpha} m_i \alpha_i$, with $m,m_i\ge 0$. Then we have that
 $r_\alpha (\gamma) = -m\alpha + \sum_{\alpha_i\neq \alpha} m_i(\alpha_i-k_i\alpha)
 \in \Delta$. If any of the $m_i$ are strictly positive, then the coefficient of
 $\alpha_i$ in $r_i(\gamma)$ is positive, so $r_\alpha(\gamma)$ must be positive
 because every root can be (uniquely) written as either a non-negative or a
 non-positive combination of the simple roots.
 \end{proof}

 \begin{proposition}\label{lec14P:tansitive}
   The group generated by simple reflections (with respect to some fixed
   $\Pi=\{\alpha_1,\dots, \alpha_n\}$) acts transitively on the set of sets of positive
   roots.
 \end{proposition}
 \begin{proof}
   It is enough to show that we can get from $\Delta^+$ to any other set of simple
   roots $\bar\Delta^+$.

   If $\bar\Delta^+$ contains $\Pi$, then $\bar\Delta^+=\Delta^+$ and we are done.
   Otherwise, there is some $\alpha_i\not\in \bar\Delta^+$ (equivalently,
   $-\alpha_i\in \bar\Delta^+$). Applying $r_i$, Lemma \ref{lec14L:key} tells us that
   \[
     \bigl|r_i(\Delta^+)\smallsetminus \bar\Delta^+\bigr|<
     \bigl|\Delta^+\smallsetminus \bar\Delta^+\bigr|.
   \]
   If we can show for any root $\alpha$ which is simple
   \emph{with respect to $r_i(\Delta)$}, that $r_\alpha$ is a product of simple
   reflections, then we are done by induction. But we have that $\alpha =
   r_i(\alpha_j)$ for some $j$, from which we get that $r_\alpha=r_ir_jr_i$.
 \end{proof}
 \begin{corollary}
   $\weyl$ is generated by simple reflections.
 \end{corollary}
 \begin{proof}
   Any root $\alpha$ is a simple root for some choice of positive roots. To see this,
   draw the hyperplane really close to the given root. Then we know that $\alpha$ is
   obtained from our initial set $\Pi$ by simple reflections. We get that if $\alpha =
   r_{i_1}\cdots r_{i_k}(\alpha_j)$, then $r_\alpha = (r_{i_1}\cdots
   r_{i_k})r_j(r_{i_1}\cdots r_{i_k})^{-1}$.
 \end{proof}
 We define the \emph{length}\index{length} of an element $w\in \weyl$ to be the
 smallest number $k$ so that $w=r_{i_1}\cdots r_{i_k}$, for some simple reflections
 $r_{i_j}$.

 Next, we'd like to prove that $\weyl$ acts \emph{simply transitively} on the set of
 sets of simple roots. To do this, we need the following lemma, which essentially says
 that if you have a string of simple reflections so that some positive root becomes
 negative and then positive again, then you can get the same element of $\weyl$ with
 fewer simple reflections.
 \begin{lemma}\label{lec14L:length}
   Let $\beta_1,\beta_2,\dots, \beta_t$ be a sequence in $\Pi$ (possibly with
   repetition) with $t\ge 2$. Let $r_i=r_{\beta_i}$. If $r_1r_2\cdots r_t(\beta_t)\in
   \Delta^+$, then there is some $s<t$ such that
   \[
        r_1\cdots r_t = r_1\cdots r_{s-1}r_{s+1}\cdots r_{t-1}.
   \]
   (Note that the right hand side omits $r_s$ and $r_t$.)
 \end{lemma}
 \begin{proof}
   Note that $\beta_t$ is positive and $r_1\cdots r_{t-1}(\beta_t)$ is negative, so
   there is a smallest number $s$ for which $r_{s+1}\cdots
   r_{t-1}(\beta_t)=\gamma$ is positive. Then $r_s(\gamma)$ is negative, so by Lemma
   \ref{lec14L:key}, we get $\gamma = \beta_s$. This gives us
   \begin{align*}
     r_s &= (r_{s+1}\cdots r_{t-1})r_t(r_{s+1}\cdots r_{t-1})^{-1}\\
     r_sr_{s+1}\cdots r_{t-1} &= r_{s+1}\cdots r_{t-1}r_t.
   \end{align*}
   Multiplying both sides of the second equation on the left by $r_1\cdots r_{s-1}$ to
   get the result.
 \end{proof}
 \begin{proposition}\label{lec14P:simply}
   $\weyl$ acts \emph{simply transitively} on the set of sets of positive roots.
 \end{proposition}
 \begin{proof}
   Proposition \ref{lec14P:tansitive} shows that the action is transitive, so we need
   only show that any $w\in \weyl$ which fixes $\Delta^+$ must be the identity
   element. If $w$ is a simple reflection, then it does not preserve $\Delta^+$. So we
   may assume that the shortest way to express $w$ as a product simple reflections
   uses at least two simple reflections, say $w = r_{i_1}\cdots r_{i_t}$. Then by
   Lemma \ref{lec14L:length}, we can reduce the length of $w$ by two, contradicting
   the minimality of $t$.
 \end{proof}
 \begin{corollary}
   The length of an element $w\in \weyl$ is exactly $\bigl|
   w(\Delta^+)\smallsetminus \Delta^+\bigr|$.
 \end{corollary}
 \begin{proof}
   By Proposition \ref{lec14P:simply}, $w$ is the unique element taking $\Delta^+$ to
   $w(\Delta^+)$. Say we are building a word, as in Proposition
   \ref{lec14P:tansitive}, to get from $\Delta^+$ to $w(\Delta^+)$. Assume we've
   already applied $r_{i_1}\cdots r_{i_k}$, and next we are going to reflect through
   $r_{i_1}\cdots r_{i_k}(\alpha_j)$. Then we will have applied the element
   \[
     (r_{i_1}\cdots r_{i_k})r_j(r_{i_1}\cdots r_{i_k})^{-1} (r_{i_1}\cdots r_{i_k}) = r_{i_1}\cdots
     r_{i_k}r_j.
   \]
   Thus, each time we reduce $\bigr| r_{i_1}\cdots r_{i_k}(\Delta^+)\smallsetminus
   w(\Delta^+)\bigr|$ by one, we add one simple reflection. This shows that we can
   express $w$ in the desired number of simple reflections.

   On the other hand, Lemma \ref{lec14L:key} tells us that for any sequence of simple
   reflections $r_{i_1}$,\dots, $r_{i_k}$, $\bigr| r_{i_1}\cdots
   r_{i_k}(\Delta^+)\smallsetminus \Delta^+\bigr|\le k$, so $w$ cannot be written as a
   product of fewer than $\bigl| w(\Delta^+)\smallsetminus \Delta^+\bigr|$ simple
   reflections.
 \end{proof}
 \index{Weyl group|)idxbf}

% Let's see how it works for $A_n$. Choose a vector $t= t_1\varepsilon_1+\cdots +
% t_{n+1} \varepsilon_{n+1}$ in such a way that $t_1>\cdots > t_{n+1}$. Then we have
% that $(\varepsilon_i - \varepsilon_j,t) = t_i-t_j$, so the positive roots are the
% $\varepsilon_i - \varepsilon_j$ for which $i< j$. Then the simple roots are $\Pi =
% \{\varepsilon_1-\varepsilon_2, \dots, \varepsilon_n - \varepsilon_{n+1}\}$
