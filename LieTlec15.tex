 \stepcounter{lecture}
 \setcounter{lecture}{15}
 \sektion{Lecture 15 - Dynkin diagrams, Classification of root systems}

 Last time, we talked about root systems $\Delta\subset \RR^n$. We constructed the
 Weyl group $\weyl$, the finite group generated by reflections. We considered $\Pi\subset
 \Delta$, the simple roots. We showed that $\Pi$ forms a basis for $\RR^n$, and that every
 root is a non-negative (or non-positive) linear combination of simple roots.

 If $\alpha,\beta\in \Pi$, then define $n_{\alpha\beta} =
 \frac{2(\alpha,\beta)}{(\beta,\beta)}$. We showed that $n_{\alpha\beta}$ is a
 non-positive integer. Since $n_{\alpha\beta}n_{\beta\alpha} = 4\cos^2
 \theta_{\alpha\beta}$, $n_{\alpha,\beta}$ can only be $0,-1,-2,$ or $-3$. If
 $n_{\alpha\beta}=0$, the two are orthogonal. If $n_{\alpha\beta}=-1$, then the angle
 must be $2\pi/3$ and the two are the same length. If $n_{\alpha\beta}=-2$, the angle
 must be $3\pi/4$ and $||\beta||=\sqrt{2}\ ||\alpha||$. If $n_{\alpha\beta}=-3$, the
 angle is $5\pi/6$ and $||\beta||=\sqrt{3}\ ||\alpha||$. Thus we get:
 \[\begin{array}{|c|c|c|c|} \hline
   n_{\beta\alpha} & n_{\alpha\beta} & \text{relationship} & \text{Dynkin picture} \\ \hline
   0 & 0 & \begin{xy}
  <1.75em,0em>:
  (0,0);
  \ar (1,0) *+!L{\alpha},
  \ar (0,0.7) *+!D{\beta},
\end{xy} & \begin{xy}
   (0,0) *+!R{\alpha} *\cir<2pt>{};
   (1,0) *+!L{\beta} *\cir<2pt>{};
 \end{xy}\\ \hline
   -1 & -1 & \begin{xy}
  <1.75em,0em>:
  (0,0);
  \ar (1,0) *+!L{\alpha},
  \ar (-.5,\halfrootthree) *+!DR{\beta},
\end{xy} & \begin{xy}
   (0,0) *+!R{\alpha} *\cir<2pt>{};
   (1,0) *+!L{\beta} *\cir<2pt>{} **@{-};
 \end{xy}\\ \hline
   -2 & -1 & \begin{xy}
  <1.75em,0em>:
  (0,0);
  \ar (1,0) *+!L{\alpha},
  \ar (-1,1) *+!R{\beta},
\end{xy} & \begin{xy}
   (0,0) *+!R{\alpha} *\cir<2pt>{};
   (1,0) *+!L{\beta} *\cir<2pt>{} **@{=} ?*@{<};
 \end{xy}\\ \hline
   -3 & -1 &  \begin{xy}
  <1.75em,0em>:
  (0,0);
  \ar (1,0) *+!L{\alpha},
  \ar (-1,0)+a(120) *+!R{\beta},
\end{xy} & \begin{xy}
   (0,0)="1" *+!R{\alpha} *\cir<2pt>{};
   (1,0)="2" *+!L{\beta} *\cir<2pt>{} **@{-} ?*@{<},
   \ar@{-} "1" *{\hspace{3pt}};"2" *{\hspace{3pt}} <1.5pt>
   \ar@{-} "1" *{\hspace{3pt}};"2" *{\hspace{3pt}} <-1.5pt>
 \end{xy}\\ \hline
 \end{array}\]
 \begin{definition}
   Given a root system, the \emph{Dynkin diagram}\index{Dynkin diagram|(idxbf} of the
   root system is obtained in the following way.  For each simple root, draw a node.
   We join two nodes by $n_{\alpha\beta}n_{\beta\alpha}$ lines. If there are two or
   three lines (i.e. if the roots are not the same length), then we draw an arrow from
   the longer root to the shorter root. (As always, the alligator eats the big one.)
 \end{definition}
 The Dynkin diagram is independent of the choice of simple roots. For any other choice
 of simple roots, there is an element of the Weyl group that translates between the
 two, and the Weyl group preserves inner products.

 We would really like to classify Dynkin diagrams. To aid the classification, we
 define an undirected version of the Dynkin diagram. Define $e_i =
 \frac{\alpha_i}{(\alpha_i,\alpha_i)^{1/2}}$, for $\alpha_i\in \Pi$. Then the number
 of lines between two vertices is $n_{\alpha_i\alpha_j}n_{\alpha_j\alpha_i} = 4\cdot
 \frac{(\alpha_i,\alpha_j)^2}{(\alpha_i,\alpha_i)(\alpha_j,\alpha_j)} = 4(e_i,e_j)^2$.
 \begin{definition}
   Given a set $\{e_1,\dots, e_n\}$ of linearly independent unit vectors in some
   Euclidean space with the property that $(e_i,e_j)\le 0$ and $4(e_i,e_j)^2\in \ZZ$
   for all $i$ and $j$, the \emph{Coxeter diagram}\index{Coxeter diagram|(idxbf}
   associated to the set is obtained in the following way. For each unit vector, draw
   a node. Join the nodes of $e_i$ and $e_j$ by $4(e_i,e_j)^2$ lines.
 \end{definition}
 Since every Dynkin diagram gives a Coxeter diagram, understanding Coxeter diagrams is
 a good start in classifying Dynkin diagrams.

 \begin{example}
   $A_n$ has $n$ simple roots, given by $\varepsilon_i - \varepsilon_{i+1}$. So the
   graphs are
 \[\text{Dynkin}\qquad \begin{xy}
   (0,0) *\cir<2pt>{};
   (1,0)  *\cir<2pt>{} **@{-};
   p+(.5,0) **@{-};
   p+(.6,0) **{\hspace{1pt}.\hspace{1pt}};
   p+(.5,0)  *\cir<2pt>{} **@{-};
   p+(1,0)  *\cir<2pt>{} **@{-};
 \end{xy} \]
\[\text{Coxeter}\qquad \begin{xy}
   (0,0) *=<0pt>{\bullet};
   (1,0)  *=<0pt>{\bullet} **@{-};
   p+(.5,0) **@{-};
   p+(.6,0) **{\hspace{1pt}.\hspace{1pt}};
   p+(.5,0)  *=<0pt>{\bullet} **@{-};
   p+(1,0)  *=<0pt>{\bullet} **@{-};
 \end{xy} \]
 \end{example}

 Let's prove some properties of Coxeter diagrams.
 \begin{itemize}
 \item[\hypertarget{CX1}{(CX1)}] A subgraph of a Coxeter diagram is a Coxeter diagram.
 This is obvious from the definition.

 \item[\hypertarget{CX2}{(CX2)}] A Coxeter diagram is acyclic.
 \begin{proof}
   Let $e_1,\dots, e_k$ be a cycle in the Coxeter diagram. Then
   \[
     \Bigl(\sum e_i, \sum e_i\Bigr) = k + \sum_{\substack{i< j\\ i,j\text{ adjacent}}}
     \underbrace{2(e_i,e_j)}_{\le {-1}} \le 0
   \]
   which contradicts that the inner product is positive definite.
 \end{proof}

 \item[\hypertarget{CX3}{(CX3)}] The degree of each vertex in a Coxeter diagram is
 less than or equal to 3, where double and triple edges count as two and
 three edges, respectively.
 \begin{proof}
   Let $e_0$ have $e_1,\dots, e_k$ adjacent to it. Since there are no cycles,
   $e_1,\dots, e_k$ are orthogonal to each other. So we can compute
   \begin{align*}
     \left(e_0 - \sum (e_0,e_i) e_i, e_0 - \sum (e_0,e_i) e_i \right) & > 0\\
       1- \sum (e_0,e_i)^2 &> 0
   \end{align*}
   but $(e_0,e_i)^2$ is one fourth of the number of edges connecting $e_0$ and $e_i$.
   So $k$ cannot be bigger than 3.
 \end{proof}

 \item[\hypertarget{CX4}{(CX4)}] Suppose a Coxeter diagram has a subgraph of type
 $A_n$, and only the endpoints of this subgraph have additional edges (say $\Gamma_1$
 at one end and $\Gamma_2$ at the other end). Then we can ``contract'' the stuff in
 the middle and just fuse $\Gamma_1$ with $\Gamma_2$, and the result is a Coxeter
 diagram.
 \[
  \ifthenelse{\boolean{lilbook}}{
  \begin{xy}
   (0,0)="G1" *+{\Gamma_1} *\cir{};
   (1,0)="1" *+!D{e_1} *=<0pt>{\bullet};
   p+(.5,0) **@{-};
   p+(.6,0) **{\hspace{1pt}.\hspace{1pt}};
   p+(.5,0)="2" *=<0pt>{\bullet} **@{-} *+!D{e_k};
   p+(1,0)="G2" *+{\Gamma_2} *\cir{};
   "G1"+(.225,.225); "1" **@{-}; "G1"+(.225,-.225) **@{-},
   "G2"+(-.225,-.225); "2" **@{-}; "G2"+(-.225,.225) **@{-}
 \end{xy} \qquad \longrightarrow \qquad
  \begin{xy}
   (0,0)="G1" *+{\Gamma_1} *\cir{};
   (1,0)="1" *=<0pt>{\bullet} *+!D{e_0};
   p+(1,0)="G2" *+{\Gamma_2} *\cir{};
   "G1"+(.225,.225); "1" **@{-}; "G1"+(.225,-.225) **@{-},
   "G2"+(-.225,-.225); "1" **@{-}; "G2"+(-.225,.225) **@{-}
 \end{xy}}{
  \begin{xy}
   (0,0)="G1" *+{\Gamma_1} *\cir{};
   (1,0)="1" *+!D{e_1} *=<0pt>{\bullet};
   p+(.5,0) **@{-};
   p+(.6,0) **{\hspace{1pt}.\hspace{1pt}};
   p+(.5,0)="2" *=<0pt>{\bullet} **@{-} *+!D{e_k};
   p+(1,0)="G2" *+{\Gamma_2} *\cir{};
   "G1"+(.2,.2); "1" **@{-}; "G1"+(.2,-.2) **@{-},
   "G2"+(-.2,-.2); "2" **@{-}; "G2"+(-.2,.2) **@{-}
 \end{xy} \qquad \longrightarrow \qquad
  \begin{xy}
   (0,0)="G1" *+{\Gamma_1} *\cir{};
   (1,0)="1" *=<0pt>{\bullet} *+!D{e_0};
   p+(1,0)="G2" *+{\Gamma_2} *\cir{};
   "G1"+(.2,.2); "1" **@{-}; "G1"+(.2,-.2) **@{-},
   "G2"+(-.2,-.2); "1" **@{-}; "G2"+(-.2,.2) **@{-}
 \end{xy}}
 \]
 \begin{proof}
   Let $e_1,\dots, e_k$ be the vertices in the $A_k$. Let $e_0 = e_1+\cdots + e_k$.
   Then we can compute that $(e_0,e_0)=1$. If $e_s\in \Gamma_1$ and $e_t\in \Gamma_2$,
   we get that $(e_0,e_s)=(e_1,e_s)$ and $(e_0,e_t)=(e_k,e_t)$.
 \end{proof}
 \end{itemize}
 Thus, a connected Coxeter diagram can have at most one fork (two could be glued to
 give valence 4), at most one double edge, and if there is a triple edge, nothing else
 can be connected to it.

 So the only possible connected Coxeter diagrams (an therefore Dynkin diagrams) so far
 are of the form
 \[ \begin{xy}
   (0,0) *=<0pt>{\bullet};
   (1,-.1) *=<0pt>{\bullet} **@{-};
   p+(.5,-.05) **@{-};
   p+(.6,-.06) **{\hspace{1pt}.\hspace{1pt}};
   p+(.5,-.05) *=<0pt>{\bullet} **@{-};
   %%%%%%%%%%%%%%%%%%%%%%%
   {p+(-.5,-.05) **@{-};
   p+(-.6,-.06) **{\hspace{1pt}.\hspace{1pt}};
   p+(-.5,-.05) *=<0pt>{\bullet} **@{-};
   p+(-1,-.1) *=<0pt>{\bullet} **@{-};},
   %%%%%%%%%%%%%%%%%%%%%%%
   p+(.5,0) **@{-};
   p+(.6,0) **{\hspace{1pt}.\hspace{1pt}};
   p+(.5,0)  *=<0pt>{\bullet} **@{-};
   p+(1,0)  *=<0pt>{\bullet} **@{-};
 \end{xy}\]
 \[\begin{xy}
   (0,0) *=<0pt>{\bullet};
   (1,0)  *=<0pt>{\bullet} **@{-};
   p+(.5,0) **@{-};
   p+(.6,0) **{\hspace{1pt}.\hspace{1pt}};
   p+(.5,0)  *=<0pt>{\bullet} **@{-};
   p+(1,0)  *=<0pt>{\bullet} **@2{-};
   p+(.5,0) **@{-};
   p+(.6,0) **{\hspace{1pt}.\hspace{1pt}};
   p+(.5,0)  *=<0pt>{\bullet} **@{-};
   p+(1,0)  *=<0pt>{\bullet} **@{-};
 \end{xy}
 \]
\[ \begin{xy}
   (0,0)="1" *=<0pt>{\bullet};
   (1,0)="2" *=<0pt>{\bullet} **@{-},
   \ar@{-} "1";"2" <1.5pt>
   \ar@{-} "1";"2" <-1.5pt>
 \end{xy}\]
 Now we switch gears back to Dynkin diagrams. Note that a subgraph of a Dynkin diagram
 is a Dynkin diagram. We will calculate that some diagrams are forbidden. We label
 the vertex corresponding to $\alpha_i$ with a number $m_i$, and check that
 \[
    \left(\sum m_i \alpha_i, \sum m_i \alpha_i\right) = 0.
 \]
\[ \begin{xy}
   (0,0) *+!D{1} *\cir<2pt>{};
   (1,0) *+!D{2} *\cir<2pt>{} **@{-};
   p+(1,0) *+!D{3} *\cir<2pt>{} **@{-};
   p+(1,0) *+!D{4} *\cir<2pt>{} **@{=}; {?*@{>}},
   p+(1,0) *+!D{2} *\cir<2pt>{} **@{-};
 \end{xy} \qquad
 \begin{xy}
   (0,0) *+!D{1} *\cir<2pt>{};
   (1,0) *+!D{2} *\cir<2pt>{} **@{-};
   p+(1,0) *+!D{3} *\cir<2pt>{} **@{-};
   p+(1,0) *+!D{2} *\cir<2pt>{} **@{=}; {?*@{<}},
   p+(1,0) *+!D{1} *\cir<2pt>{} **@{-};
 \end{xy}\]
 \[\begin{xy}
   (0,0) *+!D{1} *\cir<2pt>{};
   (1,0) *+!D{2} *\cir<2pt>{} **@{-};
   p+(1,0) *+!D{3} *\cir<2pt>{} **@{-};
       {p+(0,-1) *+!L{2} *\cir<2pt>{} **@{-};
       p+(0,-1) *+!L{1} *\cir<2pt>{} **@{-};},
   p+(1,0) *+!D{2} *\cir<2pt>{} **@{-};
   p+(1,0) *+!D{1} *\cir<2pt>{} **@{-};
 \end{xy}\qquad
 \begin{xy}
   (0,0) *+!D{1} *\cir<2pt>{};
   (1,0) *+!D{2} *\cir<2pt>{} **@{-};
   p+(1,0) *+!D{3} *\cir<2pt>{} **@{-};
   p+(1,0) *+!D{4} *\cir<2pt>{} **@{-};
        p+(0,-1) *+!L{2} *\cir<2pt>{} **@{-},
   p+(1,0) *+!D{3} *\cir<2pt>{} **@{-};
   p+(1,0) *+!D{2} *\cir<2pt>{} **@{-};
   p+(1,0) *+!D{1} *\cir<2pt>{} **@{-};
 \end{xy}\]
 \[\begin{xy}
   (0,0) *+!D{2} *\cir<2pt>{};
   (1,0) *+!D{4} *\cir<2pt>{} **@{-};
   p+(1,0) *+!D{6} *\cir<2pt>{} **@{-};
       p+(0,-1) *+!L{3} *\cir<2pt>{} **@{-},
   p+(1,0) *+!D{5} *\cir<2pt>{} **@{-};
   p+(1,0) *+!D{4} *\cir<2pt>{} **@{-};
   p+(1,0) *+!D{3} *\cir<2pt>{} **@{-};
   p+(1,0) *+!D{2} *\cir<2pt>{} **@{-};
   p+(1,0) *+!D{1} *\cir<2pt>{} **@{-};
 \end{xy}\]

 Thus we have narrowed our list of possible Dynkin diagrams to a short list.

 The ``classical'' connected Dynkin diagrams are shown below ($n$ is the total number
 of vertices).
 \begin{gather*}
  \begin{xy}
    (0,0) *\cir<2pt>{};
    (1,0)  *\cir<2pt>{} **@{-};
    p+(.5,0) **@{-};
    p+(.6,0) **{\hspace{1pt}.\hspace{1pt}};
    p+(.5,0)  *\cir<2pt>{} **@{-};
    p+(1,0)  *\cir<2pt>{} **@{-};
  \end{xy} \tag{$A_n$}\\
  \begin{xy}
    (0,0) *\cir<2pt>{};
    (1,0)  *\cir<2pt>{} **@{-};
    p+(.5,0) **@{-};
    p+(.6,0) **{\hspace{1pt}.\hspace{1pt}};
    p+(.5,0)  *\cir<2pt>{} **@{-};
    p+(1,0)  *\cir<2pt>{} **@{=} ?*@{>};
  \end{xy} \tag{$B_n$}\\
  \begin{xy}
    (0,0) *\cir<2pt>{};
    (1,0)  *\cir<2pt>{} **@{-};
    p+(.5,0) **@{-};
    p+(.6,0) **{\hspace{1pt}.\hspace{1pt}};
    p+(.5,0)  *\cir<2pt>{} **@{-};
    p+(1,0)  *\cir<2pt>{} **@{=} ?*@{<};
  \end{xy} \tag{$C_n$}\\
  \begin{xy}
    (0,0) *\cir<2pt>{};
    (1,0)  *\cir<2pt>{} **@{-};
    p+(.5,0) **@{-};
    p+(.6,0) **{\hspace{1pt}.\hspace{1pt}};
    p+(.5,0)  *\cir<2pt>{} **@{-};
    p+a(25)  *\cir<2pt>{} **@{-},
    p+a(-25)  *\cir<2pt>{} **@{-};
  \end{xy} \tag{$D_n$}\\
 \end{gather*}
 The ``exceptional'' Dynkin diagrams are
 \begin{gather*}
  \begin{xy}
    (0,0)="1" *\cir<2pt>{};
    (1,0)="2" *\cir<2pt>{} **@{-} ?*@{>},
    \ar@{-} "1" *{\hspace{3pt}};"2" *{\hspace{3pt}} <1.5pt>
    \ar@{-} "1" *{\hspace{3pt}};"2" *{\hspace{3pt}} <-1.5pt>
  \end{xy} \tag{$G_2$}\\
  \begin{xy}
    (0,0) *\cir<2pt>{};
    (1,0)  *\cir<2pt>{} **@{-};
    p+(1,0) *\cir<2pt>{} **@{=}; {?*@{>}},
    p+(1,0)  *\cir<2pt>{} **@{-};
  \end{xy} \tag{$F_4$}\\
  \begin{xy}
    (0,0) *\cir<2pt>{};
    (1,0)  *\cir<2pt>{} **@{-};
    p+(1,0) *\cir<2pt>{} **@{-};
    p+(0,-1) *\cir<2pt>{} **@{-},
    p+(1,0) *\cir<2pt>{} **@{-};
    p+(1,0) *\cir<2pt>{} **@{-};
  \end{xy} \tag{$E_6$}\\
  \begin{xy}
    (0,0) *\cir<2pt>{};
    (1,0)  *\cir<2pt>{} **@{-};
    p+(1,0) *\cir<2pt>{} **@{-};
    p+(1,0) *\cir<2pt>{} **@{-};
    p+(0,-1) *\cir<2pt>{} **@{-},
    p+(1,0) *\cir<2pt>{} **@{-};
    p+(1,0) *\cir<2pt>{} **@{-};
  \end{xy} \tag{$E_7$}\\
  \begin{xy}
    (0,0) *\cir<2pt>{};
    (1,0)  *\cir<2pt>{} **@{-};
    p+(1,0) *\cir<2pt>{} **@{-};
    p+(1,0) *\cir<2pt>{} **@{-};
    p+(1,0) *\cir<2pt>{} **@{-};
    p+(0,-1) *\cir<2pt>{} **@{-},
    p+(1,0) *\cir<2pt>{} **@{-};
    p+(1,0) *\cir<2pt>{} **@{-};
  \end{xy} \tag{$E_8$}\\
 \end{gather*}
 It remains to show that each of these is indeed the Dynkin diagram of some root
 system.

 We have already constructed the root system $A_n$ in Example \ref{lec14Eg:A_n}.

 Next we construct $D_n$\index{Dn@$D_n$!construction of|idxbf}. Let
 $\varepsilon_1,\dots, \varepsilon_n$ be an orthonormal basis for $\RR^n$. Then let
 the roots be
 \[
    \Delta = \{\pm (\varepsilon_i \pm \varepsilon_j) | i< j\le n\}.
 \]
 We choose the simple roots to be
 \[\begin{xy}
   (0,0) *{\varepsilon_1 - \varepsilon_2} *+\frm{-}; (1,0) **@{-};
   (1.5,0) **{\hspace{.2mm}.\hspace{.2mm}};
   (3,0) *{\varepsilon_{n-2} - \varepsilon_{n-1}} *+\frm{-} **@{-};
   (3,-1) *{\varepsilon_{n-1} + \varepsilon_{n}} *+\frm{-} **@{-},
   (5.5,0) *{\varepsilon_{n-1} - \varepsilon_{n}} *+\frm{-} **@{-},
 \end{xy}\tag{$D_n$}\]

 To get the root system for $B_n$\index{Bn@$B_n$!construction of|idxbf}, take $D_n$
 and add $\{\pm\varepsilon_i| i\le n\}$, in which case the simple roots are
 \[\begin{xy}
   (0,0) *{\varepsilon_1 - \varepsilon_2} *+\frm{-}; (1,0) **@{-};
   (1.5,0) **{\hspace{.2mm}.\hspace{.2mm}};
   (2.8,0) *{\varepsilon_{n-1} - \varepsilon_{n}} *+\frm{-} **@{-};
   (4.5,0) *{\varepsilon_{n}} *+\frm{-} **@{=} ?<>(.5)*@{>};
 \end{xy}\tag{$B_n$}\]

 To get $C_n$\index{Cn@$C_n$!construction of|idxbf}, take $D_n$ and add $\{\pm
 2\varepsilon_i|i\le n\}$, then the simple roots are
 \[\begin{xy}
   (0,0) *{\varepsilon_1 - \varepsilon_2} *+\frm{-}; (1,0) **@{-};
   (1.5,0) **{\hspace{.2mm}.\hspace{.2mm}};
   (2.8,0) *{\varepsilon_{n-1} - \varepsilon_{n}} *+\frm{-} **@{-};
   (4.6,0) *{2\varepsilon_{n}} *+\frm{-} **@{=} ?<>(.5)*@{<};
 \end{xy}\tag{$C_n$}\]

 \begin{remark}
  Recall that we can define coroots $\check \alpha = \frac{2\alpha}{(\alpha,\alpha)}$.
  Replacing all the roots with their coroots will reverse the arrows in the Dynkin
  diagram. The dual root system\index{root system!dual} is usually the same as the
  original, but is sometimes different. For example, $C_n$ and $B_n$ are dual.
 \end{remark}

 Now let's construct the exceptional root systems.

 We constructed $G_2$ when we classified rank two root systems on page
 \pageref{lec14Rmkrank2}.

 $F_4$\index{F4@$F_4$!construction of|idxbf} comes from some special properties of a
 cube in 4-space\anton{how so?}. Let $\varepsilon_1$, $\varepsilon_2$, $\varepsilon_3$,
 $\varepsilon_4$ be an orthonormal basis for $\RR^4$. Then let the roots be
 \[
 \left\{\pm(\varepsilon_i \pm \varepsilon_j), \pm \varepsilon_i, \frac{\pm
(\varepsilon_1\pm \varepsilon_2 \pm \varepsilon_3 \pm \varepsilon_4)}{2}\right\}
 \]
% These are orthogonal to some faces of the 4-cube.

 The simple roots are
 \[\begin{xy}
   (0,0) *{\e_1-\e_2} *+\frm{-};
   p+(2,0) *{\e_2-\e_3} *+\frm{-} **@{-};
   p+(1.5,0) *{\e_3} *+\frm{-} **@{=}; {?<>(.6)*@{>}},
   p+(2,0) *{\frac{\e_1 +\e_2 +\e_3 +\e_4}{2}} *+\frm{-} **@{-};
 \end{xy}
 \tag{$F_4$}\] There are 48 roots total. Remember that the dimension of the Lie
 algebra (which we have yet to construct) is the number of roots plus the dimension of the
 Cartan subalgebra (the rank of $\g$, which is 4 here), so the dimension is 52 in this
 case.

 To construct $E_8$\index{E8@$E_8$!construction of|idxbf}, look at $\RR^9$ with our
 usual orthonormal basis. The trick is that we are going to project on to the plane
 orthogonal to $\varepsilon_1+\cdots + \varepsilon_9$. The roots are
 \[
    \{\varepsilon_i - \varepsilon_j| i\neq j\} \cup \{\pm (\varepsilon_i+\varepsilon_j + \varepsilon_k)| i\neq j\neq k\}
 \]
 The total number of roots is $|\Delta| = 9\cdot 8 + 2\binom{9}{3} = 240$. So the
 dimension of the algebra is 248! The simple roots are
 \[\hspace*{-1em}\begin{xy}
   (0,0) *{\varepsilon_1 - \varepsilon_2} *+\frm{-};
   p+(1.7,0) *{\varepsilon_2 - \varepsilon_3} *+\frm{-} **@{-};
   p+(1.7,0) *{\varepsilon_3 - \varepsilon_4} *+\frm{-} **@{-};
   p+(1.7,0) *{\varepsilon_4 - \varepsilon_5} *+\frm{-} **@{-};
   p+(1.7,0) *{\varepsilon_5 - \varepsilon_6} *+\frm{-} **@{-};
   p+(0,-1) *{\varepsilon_6 + \varepsilon_7 + \varepsilon_8} *+\frm{-} **@{-},
   p+(1.7,0) *{\varepsilon_6 - \varepsilon_7} *+\frm{-} **@{-};
   p+(1.7,0) *{\varepsilon_7 - \varepsilon_8} *+\frm{-} **@{-};
 \end{xy}\tag{$E_8$}\]

 The root systems $E_6$ and $E_7$ are contained in the obvious way in the root system
 $E_8$.
 \begin{warning}\label{lec15Warn}
   Remember to project onto the orthogonal complement of $\e_1+\cdots+\e_9$. Thus,
   $\e_6+\e_7+\e_8$ is really $\frac{2}{3}(\e_6+\e_7+\e_8) -
   \frac{1}{3}(\e_1+\cdots+\e_5 + \e_9)$. There is another way to construct this root
   system, which is discussed in Lecture 25.
 \end{warning}
 \begin{exercise}
   Verify that $F_4$ and $E_8$ are root systems, and that the given sets are simple
   roots.
   \begin{solution}
     It is immediate to verify \hyperlink{RS1}{RS1} and \hyperlink{RS3}{RS3}. One may
     check that the proposed sets of simple roots are correct by checking that every
     root can be written as a non-positive or non-negative integer combination of the
     proposed simple roots. It is not hard to verify that the given root systems
     satisfy $r_\alpha(\Delta)=\Delta$ for each $\alpha\in \Delta$.

     Finally, it is enough to verify \hyperlink{RS2}{RS2} in the case where $\beta$ is
     a simple root. Since every root is an integer sum of simple roots, it is enough
     to consider the case where $\alpha$ is also a simple root. This amounts to
     checking that the given number of lines between $\alpha$ and $\beta$ is correct,
     which is relatively straightforward (keeping in mind Warning \ref{lec15Warn}).
   \end{solution}
 \end{exercise}

 We have now classified all indecomposable root systems. The diagram of the root
 system $\Delta_1\coprod \Delta_2$ is the disjoint union of the diagrams of $\Delta_1$
 and $\Delta_2$.

 \subsektion{Construction of the Lie algebras \texorpdfstring{$A_n$}{An},
 \texorpdfstring{$B_n$}{Bn}, \texorpdfstring{$C_n$}{Cn}, and
 \texorpdfstring{$D_n$}{Dn}}

 Next lecture, we will prove Serre's Theorem (Theorem
 \ref{lec16T:Serre})\index{Serre's Theorem}, which states that for each irreducible
 root system and for each algebraically closed field of characteristic zero, there is
 a unique simple Lie algebra with the given root system (it actually gives
 explicit generators and relations for this Lie algebra). Meanwhile, we will
 explicitly construct Lie algebras with the classical root systems.

 \underline{$A_n$}: Example \ref{lec14Eg:A_n} shows that $\sl(n+1)$ has root system
 $A_n$.

 \underline{$D_n$}:\index{Dn@$D_n$!and $\so(2n)$|idxbf}\index{so(2n)@$\so(2n)$}
 Consider $\so(2n)$, the Lie algebra of linear maps of $k^{2n}$ preserving some
 non-degenerate symmetric form. We can choose a basis for $k^{2n}$ so that the matrix
 of the form is $I = \matrix 0{1_n}{1_n}0$. Let $X\in \so(2n)$, then we have that $X^t
 I+IX=0$. It follows that $X$ is of the form
 \[
    X = \mat{A & B\\ C& -A^t}\text{, with $B^t=-B, C^t=-C$.}
 \]
 We guess\footnote{This will always be the right guess. The elements of the Cartan are
 simultaneously diagonalizable, so in some basis, the Cartan is exactly the set of
 diagonal matrices in the Lie algebra. The guess would be wrong if the Lie algebra did
 not have enough diagonal elements, but this would just mean that we had chosen the
 wrong basis.} that and element $H$ of the Cartan subalgebra should have the form
 $A=diag(t_1,\dots, t_n)$ and $B=C=0$ (to check this guess, it is enough to
 demonstrate that we get a root decomposition). To compute the root spaces,
 we try bracketing $H$ with various elements of $\so(2n)$. We have that $\matrix
 {E_{ij}}00{-E_{ji}}$ has eigenvalue $t_i-t_j$, that $\matrix 0{E_{ij}-E_{ji}}00$ has
 eigenvalue $t_i+t_j$, and that $\matrix 00{E_{ij}-E_{ji}}0$ eigenvalue $-t_i-t_j$.
 Since these matrices span $\so(2n)$, we know that we are done. Thus, $D_n$ is the
 root system of $\so(2n)$.

 \underline{$B_n$}:\index{Bn@$B_n$!and $\sp(2n)$|idxbf}\index{sp(2n)@$\sp(2n)$} Consider
 $\sp(2n)$, the linear operators on $k^{2n}$ which preserve a non-degenerate
 skew-symmetric form. In some basis, the form is $J = \matrix 0{1_n}{-1_n}0$, so an
 element $X\in \sp(2n)$ satisfies $X^tJ+JX=0$. It follows that $X$ is of the form
 \[
    X = \mat{A & B\\ C& -A^t}\text{, with $B^t=B, C^t=C$.}
 \]
 Let the Cartan subalgebra be the diagonal matrices. We get all the same roots as
 for $\so(2n)$, and a few more. $\matrix 0{E_{ii}}00$ has eigenvalue $2t_i$, and
 $\matrix 00{E_{ii}}0$ has eigenvalue $-2t_i$. Thus, $B_n$ is the root system of
 $\sp(2n)$.

 \underline{$C_n$}:\index{Cn@$C_n$!and $\so(2n+1)$|idxbf}\index{so(2n+1)@$\so(2n+1)$}
 Consider $\so(2n+1)$. Choose a basis so that the non-degenerate symmetric form is
 \[I = \left(\begin{array}{c|cc}
   1 & 0 & 0\\ \hline
   0 & 0 & 1_n\\
   0 & 1_n & 0\\
 \end{array}\right).
 \]
 Then an element $X\in \so(2n+1)$, satisfying $X^tI+IX=0$, has the form
 \[
  X = \left(\begin{array}{c|cc}
   0 & u & v \\ \hline
   -v^t & A & B\\
   -u^t & C & -A^t
  \end{array}\right)\text{, with $B^t=-B, C^t=-C$,}
 \]
 where $u$ and $v$ are row vectors of length $n$. Again, we take the Cartan subalgebra
 to be the diagonal matrices. We get all the same roots as we for $\so(2n)$, and a few
 more. If $e_i$ is the row vector with a one in the $i$-th spot and zeros elsewhere,
 then {\scriptsize $\left(\begin{array}{c|cc}
   0 & e_i & 0\\ \hline
   0 & 0 & 0 \\
   -e_i^t & 0 & 0
 \end{array}\right)$}
 has eigenvalue $t_i$, and
 {\scriptsize $\left(\begin{array}{c|cc}
   0 & 0 & e_i \\ \hline
   -e_i^t & 0 & 0 \\
   0 & 0 & 0
 \end{array}\right)$}
 has eigenvalue $-t_i$. Thus, $C_n$ is the root system of $\so(2n+1)$.

 \subsektion{Isomorphisms of small dimension} Let's say that we believe Serre's
 Theorem. Then you can see that for small $n$ some of the Dynkin diagrams coincide, so
 the corresponding Lie algebras are isomorphic.
 \[\begin{array}{ccc}
 B_2 = C_2 & D_2 = A_1\coprod A_1 & D_3=A_3 \\
 \begin{xy}
   (0,0) *{},
   (0,.5) *\cir<2pt>{};
   (1,.5) *\cir<2pt>{} **@{=} ?*@{>},
 \end{xy} & \begin{xy}
   (0,.1) *\cir<2pt>{},
   (0,.9) *\cir<2pt>{},
 \end{xy} &
 \begin{xy}
   (0,0) *\cir<2pt>{};
   a(30) *\cir<2pt>{} **@{-};
   (0,1) *\cir<2pt>{} **@{-},
 \end{xy}\\
 \quad \so(5)\simeq \sp(4) \quad & \quad \so(4)\simeq \sl(2)\times \sl(2)
 \quad & \quad \so(6)\simeq \sl(4) \quad
 \end{array}\]
 We can see some of these isomorphisms directly on the level of groups! Let's
 construct a map of groups $SL(2)\times SL(2) \to SO(4)$, whose kernel is discrete.
 Let $W$ be the space of $2\times 2$ matrices, then the $SL(2)\times SL(2)$ acts on
 $W$ by $(X,Y)w = XwY^{-1}$. This action preserves the determinant of $w=\matrix
 abcd$. That is, the quadratic form $ad-bc$ is preserved, so the corresponding
 non-degenerate bilinear form is preserved.\footnote{Given a quadratic form $Q$, one
 gets a symmetric bilinear form $(w,w'):=Q(w+w')-Q(w)-Q(w')$. In the case $Q(w)=\det
 w$, the form is non-degenerate. Indeed, assume $\det(w+w')=\det w + \det w$ for all
 $w$. Then by choosing a basis so that $w'$ is in Jordan form and letting $w$ vary
 over diagonal matrices, we see that $w'=0$.} The Lie group preserving such a form is
 $SO(4)$, so we have a map $SL_(2)\times SL(2)\to SO(4)$. It is easy to check that the
 kernel is the set $\{(I,I),(-I,-I)\}$, and since the domain and range each have
 dimension 6, we get $SL(2)\times SL(2)/(\pm I,\pm I)\cong SO(4)$ (we are also using
 that $SO(4)$ is connected). This yields an isomorphism on the level of Lie algebras.

 Now let's see that $\so(6)\simeq \sl(4)$. The approach is the same. Let $V$ be the
 standard 4 dimensional representation of $SL(4)$. Let $W = \Lambda^2 V$, which is a
 6 dimensional representation of $SL(4)$. Note that you have a pairing
 \[
    W\times W = \Lambda^2 V \times \Lambda^2 V \to \Lambda^4 V \overset{\det}{\simeq} k
 \]
 where the last map is an isomorphism of representations of $SL(4)$ (because the
 determinant of any element of $SL(4)$ is 1). Thus, $W=\Lambda^2 V$ has some
 $SL(4)$-invariant non-degenerate symmetric bilinear form, so we have a map $SL(4)\to
 SO(W)\simeq SO(6)$. It is not hard to check that the kernel is $\pm I$, and the
 dimensions match, so we get an isomorphism of Lie algebras.
